{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Machine learning techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rd\n",
    "rd.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1 Loading and exploring the dataset\n",
    "\n",
    "First, we use pandas to load the dataset from a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('./titanic.csv')\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can explore the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining the length of the dataset\n",
    "print(\"The dataset has\", len(raw_data), \"rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining the columns in the dataset\n",
    "print(\"Columns (features of the dataset)\")\n",
    "raw_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining the labels\n",
    "print(\"Labels\")\n",
    "raw_data[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining how many passengers survived\n",
    "print(sum(raw_data['Survived']),'passengers survived out of',len(raw_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One can look at several columns together\n",
    "raw_data[[\"Name\", \"Age\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2. Cleaning up the data\n",
    "\n",
    "Now, let's look at how many columns have missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Cabin column is missing too many values to be useful. Let's drop it altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data['Cabin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The Cabin column is missing\", sum(raw_data['Cabin'].isna()), \"values out of\",len(raw_data['Cabin']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = raw_data.drop('Cabin', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other columns such as Age or Embarked are missing some values, but they can still be useful.\n",
    "\n",
    "For the age column, let's fill in the missing values with the median of all ages.\n",
    "\n",
    "For the Embarked column, let's make a new category called 'U', for Unknown port of embarkment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data['Age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_age = raw_data[\"Age\"].median()\n",
    "median_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data[\"Age\"] = clean_data[\"Age\"].fillna(median_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data[\"Embarked\"] = clean_data[\"Embarked\"].fillna('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.2.3 Saving our data for the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.to_csv('./clean_titanic_data.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.3 Manipulating the features\n",
    "\n",
    "- One-hot encoding\n",
    "- Binning\n",
    "- Feature selection\n",
    "\n",
    "### 12.3.1 One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = pd.read_csv('clean_titanic_data.csv')\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_columns = pd.get_dummies(preprocessed_data['Sex'], prefix='Sex')\n",
    "print(gender_columns)\n",
    "embarked_columns = pd.get_dummies(preprocessed_data[\"Embarked\"], prefix=\"Pclass\")\n",
    "print(embarked_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = pd.concat([preprocessed_data, gender_columns], axis=1)\n",
    "preprocessed_data = pd.concat([preprocessed_data, embarked_columns], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = pd.concat([preprocessed_data, gender_columns], axis=1) \n",
    "preprocessed_data = pd.concat([preprocessed_data, embarked_columns], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = preprocessed_data.drop(['Sex', 'Embarked'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.2 Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 10, 20, 30, 40, 50, 60, 70, 80]\n",
    "categorized_age = pd.cut(preprocessed_data['Age'], bins)\n",
    "preprocessed_data['Categorized_age'] = categorized_age\n",
    "preprocessed_data = preprocessed_data.drop([\"Age\"], axis=1)\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cagegorized_age_columns = pd.get_dummies(preprocessed_data['Categorized_age'], prefix='Categorized_age')\n",
    "preprocessed_data = pd.concat([preprocessed_data, cagegorized_age_columns], axis=1)\n",
    "preprocessed_data = preprocessed_data.drop(['Categorized_age'], axis=1)\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.3 Features that can be numerical or categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_survived = preprocessed_data[['Pclass', 'Survived']]\n",
    "\n",
    "first_class = class_survived[class_survived['Pclass'] == 1]\n",
    "second_class = class_survived[class_survived['Pclass'] == 2]\n",
    "third_class = class_survived[class_survived['Pclass'] == 3]\n",
    "\n",
    "print(\"In first class\", sum(first_class['Survived'])/len(first_class)*100, \"% of passengers survived\")\n",
    "print(\"In second class\", sum(second_class['Survived'])/len(first_class)*100, \"% of passengers survived\")\n",
    "print(\"In third class\", sum(third_class['Survived'])/len(first_class)*100, \"% of passengers survived\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding the Pclass feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorized_pclass_columns = pd.get_dummies(preprocessed_data['Pclass'], prefix='Pclass')\n",
    "preprocessed_data = pd.concat([preprocessed_data, categorized_pclass_columns], axis=1)\n",
    "preprocessed_data = preprocessed_data.drop(['Pclass'], axis=1)\n",
    "preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.4 Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data = preprocessed_data.drop(['Name', 'Ticket', 'PassengerId'], axis=1)\n",
    "preprocessed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3.5 Saving for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_data.to_csv('./preprocessed_titanic_data.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.4 Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./preprocessed_titanic_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.1 Features-labels split and train-validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.drop([\"Survived\"], axis=1)\n",
    "labels = data[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remark: we fix random_state the end, to make sure we always get the same results\n",
    "features_train, features_validation_test, labels_train, labels_validation_test = train_test_split(\n",
    "    features, labels, test_size=0.4, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_validation, features_test, labels_validation, labels_test = train_test_split(\n",
    "    features_validation_test, labels_validation_test, test_size=0.5, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(features_train))\n",
    "print(len(features_validation))\n",
    "print(len(features_test))\n",
    "print(len(labels_train))\n",
    "print(len(labels_validation))\n",
    "print(len(labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.2 Training different models on our dataset\n",
    "\n",
    "We'll train four models:\n",
    "- Logistic regression (perceptron)\n",
    "- Decision tree\n",
    "- Naive Bayes\n",
    "- Support vector machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_model.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_model = GradientBoostingClassifier()\n",
    "gb_model.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ab_model = AdaBoostClassifier()\n",
    "ab_model.fit(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.3 Evaluating the models\n",
    "\n",
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scores of the models\")\n",
    "print(\"Logistic regression:\", lr_model.score(features_validation, labels_validation))\n",
    "print(\"Decision tree:\", dt_model.score(features_validation, labels_validation))\n",
    "print(\"Naive Bayes:\", nb_model.score(features_validation, labels_validation))\n",
    "print(\"SVM:\", svm_model.score(features_validation, labels_validation))\n",
    "print(\"Random forest:\", rf_model.score(features_validation, labels_validation))\n",
    "print(\"Gradient boosting:\", gb_model.score(features_validation, labels_validation))\n",
    "print(\"AdaBoost:\", ab_model.score(features_validation, labels_validation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"F1-scores of the models:\")\n",
    "\n",
    "lr_predicted_labels = lr_model.predict(features_validation)\n",
    "print(\"Logistic regression:\", f1_score(labels_validation, lr_predicted_labels))\n",
    "\n",
    "dt_predicted_labels = dt_model.predict(features_validation)\n",
    "print(\"Decision Tree:\", f1_score(labels_validation, dt_predicted_labels))\n",
    "\n",
    "nb_predicted_labels = nb_model.predict(features_validation)\n",
    "print(\"Naive Bayes:\", f1_score(labels_validation, nb_predicted_labels))\n",
    "\n",
    "svm_predicted_labels = svm_model.predict(features_validation)\n",
    "print(\"Support Vector Machine:\", f1_score(labels_validation, svm_predicted_labels))\n",
    "\n",
    "rf_predicted_labels = rf_model.predict(features_validation)\n",
    "print(\"Random Forest:\", f1_score(labels_validation, rf_predicted_labels))\n",
    "\n",
    "gb_predicted_labels = gb_model.predict(features_validation)\n",
    "print(\"Gradient boosting:\", f1_score(labels_validation, gb_predicted_labels))\n",
    "\n",
    "ab_predicted_labels = ab_model.predict(features_validation)\n",
    "print(\"AdaBoost:\", f1_score(labels_validation, ab_predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4.4 Testing the model\n",
    "\n",
    "Finding the accuracy and the F1-score of the model in the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_predicted_test_labels = gb_model.predict(features_test)\n",
    "f1_score(labels_test, gb_predicted_test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.5 Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search with an rbf kernel\n",
    "\n",
    "print(\"SVM grid search with a radial basis function kernel\")\n",
    "\n",
    "# rbf, C=1, gamma=0.1\n",
    "svm_1_01 = SVC(kernel='rbf', C=1, gamma=0.1)\n",
    "svm_1_01.fit(features_train, labels_train)\n",
    "print(\"C=1, gamma=0.1\", svm_1_01.score(features_validation, labels_validation))\n",
    "\n",
    "# rbf, C=1, gamma=1\n",
    "svm_1_1 = SVC(kernel='rbf', C=1, gamma=1)\n",
    "svm_1_1.fit(features_train, labels_train)\n",
    "print(\"C=1, gamma=1\", svm_1_1.score(features_validation, labels_validation))\n",
    "\n",
    "# rbf, C=1, gamma=10\n",
    "svm_1_10 = SVC(kernel='rbf', C=1, gamma=10)\n",
    "svm_1_10.fit(features_train, labels_train)\n",
    "print(\"C=1, gamma=10\", svm_1_10.score(features_validation, labels_validation))\n",
    "\n",
    "# rbf, C=10, gamma=0.1\n",
    "svm_10_01 = SVC(kernel='rbf', C=10, gamma=0.1)\n",
    "svm_10_01.fit(features_train, labels_train)\n",
    "print(\"C=10, gamma=0.1\", svm_10_01.score(features_validation, labels_validation))\n",
    "\n",
    "# rbf, C=10, gamma=1\n",
    "svm_10_1 = SVC(kernel='rbf', C=10, gamma=1)\n",
    "svm_10_1.fit(features_train, labels_train)\n",
    "print(\"C=10, gamma=1\", svm_10_1.score(features_validation, labels_validation))\n",
    "\n",
    "# rbf, C=10, gamma=10\n",
    "svm_10_10 = SVC(kernel='rbf', C=10, gamma=10)\n",
    "svm_10_10.fit(features_train, labels_train)\n",
    "print(\"C=10, gamma=10\", svm_10_10.score(features_validation, labels_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_parameters = {'kernel': ['rbf'],\n",
    "                  'C': [0.01, 0.1, 1 , 10, 100],\n",
    "                  'gamma': [0.01, 0.1, 1, 10, 100]\n",
    "                }\n",
    "svm = SVC()\n",
    "svm_gs = GridSearchCV(estimator = svm,\n",
    "                      param_grid = svm_parameters)\n",
    "svm_gs.fit(features_train, labels_train)\n",
    "\n",
    "svm_winner = svm_gs.best_estimator_\n",
    "svm_winner\n",
    "\n",
    "svm_winner.score(features_validation, labels_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_winner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12.6 Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
